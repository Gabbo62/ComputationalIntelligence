{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 2: ES\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The goal of the game is to **avoid** taking the last object.\n",
    "\n",
    "* Task2.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task2.2: An agent using evolved rules using ES\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab2` inside your personal course repository for the course \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint, pformat\n",
    "from collections import namedtuple\n",
    "from random import random, choice, randint\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample (and silly) startegies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    \"\"\"A completely random move\"\"\"\n",
    "    row = choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive(state: Nim) -> Nimply:\n",
    "    \"\"\"A strategy that can adapt its parameters\"\"\"\n",
    "    genome = {\"love_small\": 0.5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    tmp = np.array([tuple(int(x) for x in f\"{c:032b}\") for c in state.rows])\n",
    "    xor = tmp.sum(axis=0) % 2\n",
    "    return int(\"\".join(str(_) for _ in xor), base=2)\n",
    "\n",
    "\n",
    "def analize(raw: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = dict()\n",
    "    for ply in (Nimply(r, o) for r, c in enumerate(raw.rows) for o in range(1, c + 1)):\n",
    "        tmp = deepcopy(raw)\n",
    "        tmp.nimming(ply)\n",
    "        cooked[\"possible_moves\"][ply] = nim_sum(tmp)\n",
    "    return cooked\n",
    "\n",
    "\n",
    "def optimal(state: Nim) -> Nimply:\n",
    "    analysis = analize(state)\n",
    "    logging.debug(f\"analysis:\\n{pformat(analysis)}\")\n",
    "    spicy_moves = [ply for ply, ns in analysis[\"possible_moves\"].items() if ns != 0]\n",
    "    if not spicy_moves:\n",
    "        spicy_moves = list(analysis[\"possible_moves\"].keys())\n",
    "    ply = choice(spicy_moves)\n",
    "    return ply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversimplified match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:init : <1 3 5 7 9>\n",
      "INFO:root:ply: player 0 plays Nimply(row=3, num_objects=3)\n",
      "INFO:root:status: <1 3 5 4 9>\n",
      "INFO:root:ply: player 1 plays Nimply(row=3, num_objects=2)\n",
      "INFO:root:status: <1 3 5 2 9>\n",
      "INFO:root:ply: player 0 plays Nimply(row=1, num_objects=3)\n",
      "INFO:root:status: <1 0 5 2 9>\n",
      "INFO:root:ply: player 1 plays Nimply(row=4, num_objects=5)\n",
      "INFO:root:status: <1 0 5 2 4>\n",
      "INFO:root:ply: player 0 plays Nimply(row=4, num_objects=2)\n",
      "INFO:root:status: <1 0 5 2 2>\n",
      "INFO:root:ply: player 1 plays Nimply(row=0, num_objects=1)\n",
      "INFO:root:status: <0 0 5 2 2>\n",
      "INFO:root:ply: player 0 plays Nimply(row=4, num_objects=1)\n",
      "INFO:root:status: <0 0 5 2 1>\n",
      "INFO:root:ply: player 1 plays Nimply(row=4, num_objects=1)\n",
      "INFO:root:status: <0 0 5 2 0>\n",
      "INFO:root:ply: player 0 plays Nimply(row=2, num_objects=5)\n",
      "INFO:root:status: <0 0 0 2 0>\n",
      "INFO:root:ply: player 1 plays Nimply(row=3, num_objects=1)\n",
      "INFO:root:status: <0 0 0 1 0>\n",
      "INFO:root:ply: player 0 plays Nimply(row=3, num_objects=1)\n",
      "INFO:root:status: <0 0 0 0 0>\n",
      "INFO:root:status: Player 1 won!\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "strategy = (optimal, pure_random)\n",
    "\n",
    "nim = Nim(5)\n",
    "logging.info(f\"init : {nim}\")\n",
    "player = 0\n",
    "while nim:\n",
    "    ply = strategy[player](nim)\n",
    "    logging.info(f\"ply: player {player} plays {ply}\")\n",
    "    nim.nimming(ply)\n",
    "    logging.info(f\"status: {nim}\")\n",
    "    player = 1 - player\n",
    "logging.info(f\"status: Player {player} won!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Gabriele Ferro `<gabrieleferro.00@gmail.com>`  \n",
    "[`https://github.com/Gabbo62/ComputationalIntelligence`](https://github.com/Gabbo62/ComputationalIntelligence) ~ \n",
    "[`LICENSE`](https://github.com/Gabbo62/ComputationalIntelligence/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remain_sum(state: Nim) -> int:\n",
    "    return sum(state.rows)\n",
    "\n",
    "def analize(raw: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = dict()\n",
    "    for ply in (Nimply(r, o) for r, c in enumerate(raw.rows) for o in range(1, c + 1)):\n",
    "        tmp = deepcopy(raw)\n",
    "        tmp.nimming(ply)\n",
    "        cooked[\"possible_moves\"][ply] = remain_sum(tmp)\n",
    "    return cooked\n",
    "\n",
    "def fixed_rule(state: Nim) -> Nimply:\n",
    "    analysis = analize(state)\n",
    "    logging.debug(f\"analysis:\\n{pformat(analysis)}\")\n",
    "    spicy_moves = [ply for ply, ns in analysis[\"possible_moves\"].items() if np.log2(ns+1) % 1 == 0 and ns != 0]\n",
    "    if not spicy_moves:\n",
    "        spicy_moves = list(analysis[\"possible_moves\"].keys())\n",
    "    # print(spicy_moves)\n",
    "    ply = choice(spicy_moves)\n",
    "    return ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(num_rows: int, strategy: tuple, print_log: bool = False, start_player: int = 0):\n",
    "    nim = Nim(num_rows)\n",
    "    if print_log: logging.info(f\"init : {nim}\")\n",
    "    player = start_player\n",
    "    while nim:\n",
    "        ply = strategy[player](nim)\n",
    "        if print_log: logging.info(f\"ply: player {player} plays {ply}\")\n",
    "        nim.nimming(ply)\n",
    "        if print_log: logging.info(f\"status: {nim}\")\n",
    "        player = 1 - player\n",
    "    if print_log: logging.info(f\"status: Player {player} won!\")\n",
    "    return player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.2%\n"
     ]
    }
   ],
   "source": [
    "strategy = (optimal, fixed_rule)\n",
    "\n",
    "winner = []\n",
    "num_play = 1000\n",
    "nim_rows = 5\n",
    "start_player = 0\n",
    "for _ in range(0, num_play):\n",
    "    winner.append(play(nim_rows, strategy, print_log=False, start_player=start_player))\n",
    "    start_player = 1 - start_player\n",
    "print(f'{sum(winner)/num_play*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolved Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from random import gauss\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class Nim_move:\n",
    "    fitness: int\n",
    "    move: Nimply\n",
    "    \n",
    "    def __init__(self, fitness:int, move:Nimply=None) -> None:\n",
    "        self.fitness = fitness\n",
    "        self.move = move\n",
    "\n",
    "OFFSPRING_SIZE = 100\n",
    "MUTATION_PROBABILITY = 1.0\n",
    "NGENERATION = 10\n",
    "\n",
    "def mutate(starting_state: Nim_move, state: Nim) -> Nim_move:\n",
    "    if starting_state.move is None:\n",
    "        row = choice([i for i in range(len(state.rows)) if state.rows[i] != 0])\n",
    "        take = round(state.rows[row]/2)\n",
    "    elif state.rows[starting_state.move.row] == 0:\n",
    "        row = choice([i for i in range(len(state.rows)) if state.rows[i] != 0])\n",
    "        take = round(state.rows[row]/2)\n",
    "    else:\n",
    "        row = starting_state.move.row\n",
    "        take = starting_state.move.num_objects\n",
    "\n",
    "    take = round(gauss(take, tweak_var))\n",
    "    take = min(state.rows[row], max(1, take))\n",
    "    move = Nimply(row, take)\n",
    "    state = deepcopy(state)\n",
    "    \n",
    "    state.nimming(move)\n",
    "    fitness = nim_sum(state)\n",
    "        \n",
    "    return Nim_move(fitness, move)\n",
    "\n",
    "def evolved_rule(state: Nim) -> Nimply:    \n",
    "    population = Nim_move(nim_sum(state))\n",
    "    \n",
    "    for _ in range(NGENERATION):\n",
    "        offspring = list([population] if population.move is not None else [])\n",
    "        for _ in range(OFFSPRING_SIZE):\n",
    "            if random() < MUTATION_PROBABILITY:\n",
    "                offspring.append(mutate(population, state))\n",
    "\n",
    "        population = min(offspring, key=lambda i: i.fitness)\n",
    "    return population.move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training obtained variance = 0.016131086363082886\n",
      "Training winning percentage: 51.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strategy = (optimal, evolved_rule)\n",
    "\n",
    "winner = []\n",
    "num_play = 100\n",
    "nim_rows = 5\n",
    "print_log = num_play < 10\n",
    "start_player = 0\n",
    "\n",
    "tweak_var = 0.0001\n",
    "\n",
    "for i in tqdm(range(0, num_play)):\n",
    "    current_winner = play(nim_rows, strategy, print_log=print_log, start_player=start_player)\n",
    "    winner.append(current_winner)\n",
    "        \n",
    "    if i%5 == 0 and i != 0 and sum(winner[i-5:i]) > 1:\n",
    "        tweak_var *= np.e**(1/3)\n",
    "    elif i%5 == 0 and i != 0:\n",
    "        tweak_var /= np.e**(1/12)\n",
    "    \n",
    "print(f'Training obtained variance = {tweak_var}')\n",
    "print(f'Training winning percentage: {sum(winner)/num_play*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing with variance = 0.016131086363082886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning percentage: 43.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strategy = (optimal, evolved_rule)\n",
    "\n",
    "winner = []\n",
    "num_play = 100\n",
    "nim_rows = 5\n",
    "print_log = num_play < 10\n",
    "start_player = 0\n",
    "\n",
    "print(f'Playing with variance = {tweak_var}')\n",
    "\n",
    "for _ in tqdm(range(0, num_play)):\n",
    "    current_winner = play(nim_rows, strategy, print_log=print_log, start_player=start_player)\n",
    "    winner.append(current_winner)\n",
    "    start_player = 1 - start_player\n",
    "    \n",
    "print(f'Winning percentage: {sum(winner)/num_play*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8b3145ae2f8986a382e1d29057a790087de06911369db9ba2490276da319dff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
